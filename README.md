# RACING_CAR üèéÔ∏è

##  üèÅ Description
Racing Car est un projet de Deep Reinforcement Learning (DRL) visant √† entra√Æner un agent intelligent √† ma√Ætriser un environnement de conduite.

- Algorithme : Le projet utilise l'algorithme Deep Q-Network (DQN), une technique fondamentale du DRL qui combine les Q-Learning (un algorithme d'apprentissage par valeurs) avec des r√©seaux neuronaux profonds (Deep Learning) pour g√©rer des espaces d'observation vastes (images).

- Environnement : L'agent apprend √† conduire dans l'environnement CarRacing-v3 de Gymnasium, qui simule une voiture vue de dessus sur un circuit g√©n√©r√© de mani√®re stochastique. L'agent doit apprendre √† naviguer le plus rapidement possible tout en √©vitant de sortir de piste.

## ‚≠êÔ∏è Fonctionnalit√©s et Architecture 
Cette section d√©taille les choix techniques et l'impl√©mentation sp√©cifique de l'algorithme DQN pour l'environnement CarRacing-v3.

### üß† Mod√®le d'Apprentissage  :
- **Algorithme** : Utilisation de l'algorithme Deep Q-Network (DQN) avec une architecture de r√©seaux de neurones convolutifs (CNN) pour le traitement des images.

- **Historique d'Observation** : L'entr√©e du r√©seau est un empilement de 4 frames successives ($84 \times 84 \times 4$), ce qui permet √† l'agent de percevoir le mouvement et la v√©locit√© (essentiel pour la conduite). 

- **Stabilisation** : Le mod√®le utilise un Target Network (r√©seau cible) pour stabiliser la pr√©diction des valeurs Q et une M√©moire de Replay Exp√©rience (Experience Replay Buffer) pour briser la corr√©lation entre les √©chantillons d'entra√Ænement.

### üöó Espace d'Action (Contr√¥les) :
L'agent contr√¥le la voiture via un espace d'action discret (non-continu). Il ne peut choisir qu'une seule action √† la fois parmi les 5 suivantes :

1) Acc√©l√©rer ("Avant")

2) Tourner √† "Droite"

3) Tourner √† "Gauche"

4) "Frein"

5) "Ne rien faire"

### ‚öôÔ∏è Optimisation Multiplateforme :
Le code est optimis√© pour une ex√©cution acc√©l√©r√©e sur diff√©rentes architectures mat√©rielles avec d√©tection automatique de la plateforme :

- Apple Silicon (Mac M1/M2/M3) : Utilisation de l'API Metal Performance Shaders (MPS) pour l'acc√©l√©ration GPU.

- NVIDIA (Windows/Linux) : Utilisation de l'API CUDA pour l'acc√©l√©ration GPU, lorsque disponible.

- CPU (G√©n√©rique) : Bascule automatique sur le CPU lorsque ni MPS ni CUDA n'est d√©tect√©.

## üõ†Ô∏è Pr√©requis et installation

### ‚öôÔ∏è Pr√©requis Logiciels : 
Assurez vous d'avoir : 
   - Python 3.11 (pour meilleure compatibilit√© avec Pytorch et Gymnasium)

### üçé Pr√©requis Sp√©cifiques √† macOS (Apple Silicon) : 
Si vous rencontrez des probl√®mes lors de l'installation des paquets gymnasium[box2d], vous pourriez avoir besoin d'installer des biblioth√®ques via Homebrew (le gestionnaire de paquets macOS) :

1. SWIG : N√©cessaire pour compiler la partie Box2D de Gymnasium.

```bash
brew install swig
````

2. LTS (Library of Tensors and Streams) : G√©n√©ralement n√©cessaire pour les d√©pendances de rendu graphique comme Pygame, qui est requis par gymnasium[box2d].

```bash
brew install sdl2 sdl2_image sdl2_mixer sdl2_ttf
```
### üíø Cloner le d√©p√¥t :
   - Avec HTTPS : 
   ```bash
   git clone https://github.com/Belou25/racing_car.git
   ```
   - Avec SSH : 
   ```bash 
    git clone git@github.com:Belou25/racing_car.git
   ```

### üêç Installation des D√©pendances Python :

Une fois les pr√©requis syst√®me install√©s (si n√©cessaire), vous pouvez installer toutes les biblioth√®ques Python :

1. Cr√©ez et activez un environnement virtuel (recommand√©) :

```bash
python3.11 -m venv env
source env/bin/activate
```

2. Installez les biblioth√®ques √† partir du fichier requirements.txt :
```bash
pip install -r requirements.txt
```

## üïπÔ∏è Utilisation et Entra√Ænement
### Entra√Æner l'Agent
Pour d√©marrer l'entra√Ænement du mod√®le DQN : 
```bash
python DQN_agent_train.py
```

### Visualiser un Agent Entrain√© : 
Pour lancer l'environnement avec la visualisation (render_mode="human") et tester un mod√®le pr√©-entra√Æn√© :

```bash 
python DQN_agent_test.py
```


## üèõÔ∏è Structure du projet
- `save_graph/`: dossier contenant les graphiques de suivi de score et de tuile visit√©s par la voiture pour chaque √©pisode.
- `weights_save/`: dossier contenant les poids du mod√®les entrain√©es (mis √† jour tous les 25 √©pisodes).
- `weights_save/model3_weights.pth`: poids d'un mod√®le entrain√© fonctionnel. 
- `DQN_agent_test.py`: fichier python pour visualisation du mod√®le entra√Æn√©.
- `DQN_agent_train.py`: fichier python pour entra√Ænement du mod√®le. 
- `requirements.txt` : liste des frameworks utiles pour l'environemment. 


## üìà R√©sultats 
- **Graphiques de Convergence** : 
![Graphique score du mod√®le par √©pisodes](save_graph/racing_car3_score.png)
![Graphique nombre de tuile visit√©es par √©pisode](save_graph/racing_car3_tuile.png)

- **Vid√©o de d√©monstration du mod√®le sur 9 circuits al√©atoires** :
[![Vid√©o de d√©monstration du mod√®le](https://img.youtube.com/vi/nw7CA66YZHk/hqdefault.jpg)](https://www.youtube.com/watch?v=nw7CA66YZHk)


## üíæ D√©tails de l'Entra√Ænement Initial (Poids Fournis)
Les poids du mod√®le pr√©-entra√Æn√© (`model3_weights.pth`) fournis dans ce d√©p√¥t ont √©t√© g√©n√©r√©s dans l'environnement mat√©riel suivant :

- GPU : NVIDIA GeForce RTX 3070 (version portable)

- VRAM D√©d√©e : 8 Go

- RAM Syst√®me : 16 Go

- P√©riode d'Entra√Ænement : 24 heures cons√©cutives (environ 1500 √©pisodes) sur une seule session.

- Batch Size Utilis√© : L'entra√Ænement a √©t√© effectu√© avec un petit batch size de 32 pour rester dans la limite des 8 Go de VRAM et garantir la stabilit√©.


## üí° Recommendation pour un Nouvel Entra√Ænement 

Pour quiconque souhaite r√©entra√Æner l'agent sur une machine diff√©rente, il est essentiel d'ajuster les hyperparam√®tres et de surveiller l'utilisation du mat√©riel pour maximiser l'efficacit√©.

1. Adapter la Taille du Batch (`Batch Size`)

La taille du batch de 32, utilis√©e lors de l'entra√Ænement initial, est conservatrice. Si votre machine le permet (plus de 8 Go de VRAM), il est fortement recommand√© d'augmenter le `Batch Size` pour acc√©l√©rer la convergence.

2. Ajuster le Taux d'Apprentissage (`Learning Rate`)

Lorsque vous augmentez le `Batch Size`, le gradient de la fonction de perte devient plus stable et pr√©cis. Pour exploiter cette pr√©cision, vous devez augmenter le `Learning Rate` ($\alpha$) pour √©viter une convergence trop lente. 

3. Surveillance de la Saturation GPU/VRAM

Pour vous assurer que vous utilisez au maximum la puissance de votre carte graphique sans d√©passer la m√©moire, vous devez viser une saturation du GPU/VRAM de 90% ou plus.

- Windows : Gestionnaire des t√¢ches.
- MacOS :
```bash 
sudo powermetrics --samplers cpu_power,gpu_power -i 500
```

## üôã‚Äç‚ôÇÔ∏è Auteur
- Erwan GOURIOU
